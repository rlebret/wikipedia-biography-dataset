{
  "name": "Wikipedia-biography-dataset",
  "tagline": "This dataset gathers 728,321 biographies from wikipedia. It aims at evaluating text generation algorithms. For each article, we provide the first paragraph and the infobox (both tokenized). ",
  "body": "# WikiBio (wikipedia biography dataset)\r\nThis dataset gathers 728,321 biographies from wikipedia. It aims at evaluating text generation algorithms. For each article, we provide the first paragraph and the infobox (both tokenized). It was used in our\r\nwork,\r\n\r\n**Neural Text Generation from Structured Data with Application to the Biography Domain**<br>\r\nRÃ©mi Lebret, David Grangier and Michael Auli, EMNLP 16,<br>\r\nhttp://arxiv.org/abs/1603.07771<br>\r\n\r\nThis publication provides further information about the data and we kindly ask\r\nyou to cite this paper when using the data. The data was extracted from the\r\nEnglish wikipedia dump (enwiki-20150901) relying on the articles refered by\r\nWikiProject Biography [1].\r\n\r\nFor each article, we extracted the first paragraph (text), the infobox\r\n(structured data). Each infobox is encoded as a list of (field name, field\r\nvalue) pairs. We used Stanford CoreNLP [2] to preprocess the data, i.e. we\r\nbroke the text into sentences and tokenized both the text and the field\r\nvalues. The dataset was randomly split in three subsets train (80%), valid\r\n(10%), test (10%). We strongly recommend using test only for the final\r\nevaluation.\r\n\r\nThe data is organised in three subdirectories for train, valid and test.\r\nEach directory contains 7 files<br>\r\n<br>\r\nSET.id contains the list of wikipedia ids, one article per line.<br>\r\nSET.url contains the url of the wikipedia articles, one article per line.<br>\r\nSET.box contains the infobox data, one article per line.<br>\r\nSET.nb contains the number of sentences per article, one article per line.<br>\r\nSET.sent contains the sentences, one sentence per line.<br>\r\nSET.title contains the title of the wikipedia article, one per line.<br>\r\nSET.contributors contains the url of the wikipedia article history, which list\r\nthe authors of the article.<br>\r\n\r\nHence all the file allows to access the information for one article relying\r\non line numbers. It is necessary to use SET.nb to split the sentences\r\n(SET.sent) per article. The format for encoding the infobox data SET.box\r\nfollows the following scheme: each line encode one box, each box is encoded\r\nas a list of tab separated tokens, each token has the following form\r\nfieldname_position:wordtype. We also indicates when a field is empty or\r\ncontains no readable tokens with fieldname:<none>. For instance the first\r\nbox of the valid set starts with\r\n\r\ntype_1:pope name_1:michael  name_2:iii      name_3:of\r\nname_4:alexandria title_1:56th    title_2:pope    title_3:of      title_4:alexandria\r\ntitle_5:&       title_6:patriarch       title_7:of      title_8:the\r\ntitle_9:see       title_10:of     title_11:st.    title_12:mark   image:<none>\r\n\r\nwhich indicates that the field \"type\" contains 1 token \"pope\",\r\nthe field \"name\" contains 4 tokens \"michael iii of alexandria\",\r\nthe field \"title\" contains 12 tokens \"56th pope of alexandria &\r\npatriarch of the see of st. mark\", the field \"image\" is empty.\r\n\r\n[1] https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Biography<br>\r\n[2] http://stanfordnlp.github.io/CoreNLP/\r\n\r\n\r\n## Version Information\r\nv1.0 (this version) Initial Release.\r\n\r\n## License\r\nLicense information is provided in License.txt\r\n\r\n## Decompressing zip files\r\n\r\nWe splitted the archive in multiple files. To extract, run<br>\r\ncat wikipedia-biography-dataset.z?? > tmp.zip<br>\r\nunzip tmp.zip<br>\r\nrm tmp.zip<br>\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}